# Configuração Data Lake com Multi-Cluster Hadoop na AWS

# Nome DNS dos servidores
Namenode - ec2-34-232-63-146.compute-1.amazonaws.com
Datanode1 - ec2-34-204-168-76.compute-1.amazonaws.com
Datanode2 - ec2-34-239-180-170.compute-1.amazonaws.com


# Atualiza o SO em todos os servidores
sudo apt-get update && sudo apt-get -y dist-upgrade


# Instala o Java em todos os servidores
sudo apt-get install default-jdk -y


# Instala o Hadoop em todos os servidores
mkdir server
cd server
wget http://apache.forsale.plus/hadoop/common/hadoop-2.8.2/hadoop-2.8.3.tar.gz
tar xvzf hadoop-2.8.3.tar.gz


# Edita as variáveis de ambiente no Master (Namenode)
Editar .bashrc


# Java
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
export PATH="$PATH:$JAVA_HOME/bin"

# Hadoop
export HADOOP_HOME=/home/ubuntu/server/hadoop-2.8.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin


# Em cada um dos servidores, editar o arquivo abaixo e adicionar o JAVA_HOME:

nano /home/ubuntu/server/hadoop-2.8.3/etc/hadoop/hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64


# Em cada um dos servidores, editar o arquivo abaixo e adicionar o bloco configuration:

nano /home/ubuntu/server/hadoop-2.8.3/etc/hadoop/core-site.xml

<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://ec2-34-232-63-146.compute-1.amazonaws.com:9000</value>
    </property>
    <property>
      <name>hadoop.tmp.dir</name>
      <value>/home/ubuntu/server/hadoop-2.8.3/tmp</value>
      <description>Folder base.</description>
    </property>
</configuration>



# No Namenode:

nano /home/ubuntu/server/hadoop-2.8.3/etc/hadoop/hdfs-site.xml

<configuration>
  <property>
       <name>dfs.replication</name>
       <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/home/ubuntu/server/hadoop-2.8.3/namenode</value>
  </property>
</configuration>



# No Datanode:

nano /home/ubuntu/server/hadoop-2.8.3/etc/hadoop/hdfs-site.xml

<configuration>
  <property>
       <name>dfs.replication</name>
       <value>2</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/home/ubuntu/server/hadoop-2.8.3/datanode</value>
  </property>
</configuration>


# No Namenode:

nano /home/ubuntu/server/hadoop-2.8.3/etc/hadoop/yarn-site.xml

<configuration>
  <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
  </property>
  <property>
      <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>ec2-34-232-63-146.compute-1.amazonaws.com</value>
      <description>Hostname do Namenode.</description>
  </property>
</configuration>


# No Namenode:

cp  /home/ubuntu/server/hadoop-2.8.3/etc/hadoop/mapred-site.xml.template /home/ubuntu/server/hadoop-2.8.3/etc/hadoop/mapred-site.xml
nano /home/ubuntu/server/hadoop-2.8.3/etc/hadoop/mapred-site.xml

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>



# Configurar o SSH para Hadoop

# Em todos os servidores:

ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa

Copiar o conteúdo do arquivo id_rsa.pub em todos os servidores e em cada servidor, executar:

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


# No namenode, formatar o HDFS:

hdfs namenode -format


# No Namenode definir quem é o Master e quem é o Slave. Editar os arquivos abaixo e incluir os nomes DNS dos respectivos servidores:

nano server/hadoop-2.8.3/etc/hadoop/masters
nano server/hadoop-2.8.3/etc/hadoop/slaves


# Inicializar o Hadoop

start-dfs.sh
start-yarn.sh

stop-dfs.sh
stop-yarn.sh

Digitar jps e confirmar os serviços em execução


# Criar diretórios no Namenode e no Datanode (se necessário)

mkdir /home/ubuntu/server/hadoop-2.8.3/namenode
mkdir /home/ubuntu/server/hadoop-2.8.3/datanode


# Usando o DataLake

hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/ubuntu

hdfs dfs -copyFromLocal /tmp/texto.csv /user/ubuntu/text.csv
hdfs dfs -copyToLocal /user/ubuntu/texto.txt /tmp/


# Acesso via browser:

ec2-13-58-21-129.us-east-2.compute.amazonaws.com:50070

